{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手撕：多层 MLP + 激活函数 + 前向传播 + 反向传播 + 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0,Loss:17.268411061762947\n",
      "Epoch:100,Loss:0.9615527036924175\n",
      "Epoch:200,Loss:0.360713848580816\n",
      "Epoch:300,Loss:0.23310955439227385\n",
      "Epoch:400,Loss:0.18711196944197891\n",
      "Epoch:500,Loss:0.17493387668927263\n",
      "Epoch:600,Loss:0.1690702206898368\n",
      "Epoch:700,Loss:0.1649386811125157\n",
      "Epoch:800,Loss:0.16256309176104078\n",
      "Epoch:900,Loss:0.1606459903567892\n",
      "Epoch:999,Loss:0.15882770556443898\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# relu激活函数以及其导数\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "def relu_derivative(x):\n",
    "    return (x>0).astype(float)\n",
    "# 损失函数以及其导数\n",
    "def mse(y_true,y_pre):\n",
    "    return np.mean((y_true-y_pre)**2)\n",
    "# 损失函数的导数用于反向传播\n",
    "def mse_derivative(y_true,y_pre):\n",
    "    return 2*(y_pre-y_true)/y_true.shape[0]\n",
    "\n",
    "# 初始化网络参数\n",
    "def initial_parameters(layer_dims):\n",
    "    # 设置随机种子，使每次初始化结果都一样，方便调试和复现模型训练\n",
    "    np.random.seed(42)\n",
    "    # 储存元素\n",
    "    parameters={}\n",
    "    # layer_dims[i-1]: 前一层的节点数\n",
    "    # layer_dims[i]: 当前层的节点数\n",
    "    # np.random.randn(...): 生成标准正态分布的随机值\n",
    "    # * 0.1: 缩小初始权重值，避免初始梯度过大（有助于稳定训练）\n",
    "    for i in range (1,len(layer_dims)):\n",
    "        parameters['W'+str(i)]=np.random.randn(layer_dims[i-1],layer_dims[i])*0.1\n",
    "        # 偏置初始化为零，形状为 (1, 当前层神经元数)，即每个神经元一个偏置。\n",
    "        parameters['b'+str(i)]=np.zeros((1,layer_dims[i]))\n",
    "    return parameters\n",
    "\n",
    "# 前向传播\n",
    "def forward(x,parameters):\n",
    "    # parameters: 存储所有层参数的字典（包含权重矩阵 W1, W2, ..., WL 和偏置向量 b1, b2, ..., bL）\n",
    "    cache={'A0':x}\n",
    "    # 用 cache 字典保存每一层的中间计算结果：包括激活值 A 和线性变换结果 Z\n",
    "    # 每一层都有一个 W 和 b，所以总参数数是 2L。这里计算出有多少层（不包括输入层）。\n",
    "    L=len(parameters)//2\n",
    "\n",
    "    for i in range (1,L+1):\n",
    "        # 因为要到 L 层，所以得 L+1\n",
    "        # 线性变换\n",
    "        Z=np.dot(cache['A'+str(i-1)],parameters['W'+str(i)])+parameters['b'+str(i)]\n",
    "        if i<L:\n",
    "            # 隐藏层使用 ReLU 激活函数\n",
    "            A=relu(Z)\n",
    "        else:\n",
    "            # 输出层不使用激活函数（回归）\n",
    "            A=Z\n",
    "        cache['A'+str(i)]=A\n",
    "        cache['Z'+str(i)]=Z\n",
    "    return cache['A'+str(i)],cache\n",
    "\n",
    "# 反向传播\n",
    "def backward_propagation(y_true, parameters, cache):\n",
    "    # grads: 储存每一层的梯度，如 dW1, db1, …\n",
    "    grads={}\n",
    "    # L: 总层数（每层有 W 和 b，所以除以 2）\n",
    "    L=len(parameters)//2\n",
    "    # 求输出层的导数，也就是 mse 的导数\n",
    "    # 损失函数的梯度（对输出层激活 A 的导数）\n",
    "    # cache['A'+str(L)]是最后一层的输出\n",
    "    dA=mse_derivative(y_true,cache['A'+str(L)])\n",
    "    for i in reversed(range(1,L+1)):\n",
    "        dZ=dA\n",
    "        if i<L:\n",
    "            # 对隐藏层：激活函数是 ReLU\n",
    "            dZ=dA*relu_derivative(cache['Z'+str(i)])\n",
    "        dW=np.dot(cache['A'+str(i-1)].T,dZ)\n",
    "        #  keepdims=True：保留二维形状 (1, n_l)\n",
    "        db=np.sum(dZ, axis=0, keepdims=True)\n",
    "        dA=np.dot(dZ,parameters['W'+str(i)].T)\n",
    "        grads['dW'+str(i)]=dW\n",
    "        grads['db'+str(i)]=db\n",
    "    \n",
    "    return grads\n",
    "\n",
    "# 参数更新\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L=len(parameters)//2\n",
    "    for i in range (1,L+1):\n",
    "        # 跟新参数\n",
    "        parameters['W'+str(i)]=parameters['W'+str(i)]-learning_rate*grads['dW'+str(i)]\n",
    "        parameters['b'+str(i)]=parameters['b'+str(i)]-learning_rate*grads['db'+str(i)]\n",
    "    return parameters\n",
    "\n",
    "# 训练模型\n",
    "def train_model(X, y, hidden_layers=[10, 20, 10], learning_rate=0.01, epochs=1000):\n",
    "    # 得到参数的维度\n",
    "    n_features=X.shape[1]\n",
    "    # 增加输入层+隐藏层+输出层\n",
    "    layer_dims=[n_features]+hidden_layers+[1]\n",
    "    # 初始化参数\n",
    "    parameters=initial_parameters(layer_dims)\n",
    "    for i in range(epochs):\n",
    "        # 得到输出值和中间参数的储存\n",
    "        y_pre,cache=forward(X,parameters)\n",
    "        # 计算损失\n",
    "        loss=mse(y,y_pre)\n",
    "        # 反向传播计算梯度\n",
    "        grad=backward_propagation(y,parameters,cache)\n",
    "        # 更新参数\n",
    "        parameters=update_parameters(parameters,grad,learning_rate)\n",
    "        # 打印\n",
    "        if i %100 ==0 or i == epochs-1:\n",
    "            print(f'Epoch:{i},Loss:{loss}')\n",
    "    return parameters\n",
    "\n",
    "# 示例数据\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 3)\n",
    "true_weights = np.array([[2.0], [-3.0], [1.5]])\n",
    "y = X @ true_weights + 0.5 * np.random.randn(100, 1)\n",
    "\n",
    "# 训练 MLP\n",
    "trained_params = train_model(X, y, hidden_layers=[16, 8], learning_rate=0.01, epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
